---
layout:	post
title: "记一次线上bug定位，结果出乎意料"
date:  2022-06-09
category: 原创
tags: ["问题定位","bug"]
---

最近协助团队成员定位并解决了一个遗留系统的线上 bug，定位原因用了一个多小时，最后在某条复杂的烂 SQL 语句中加了关键字 or，两个字母，解决了 bug。下面记录下我从接到问题反馈，到定位问题，最后解决问题的过程。

## 系统现状

我对这个遗留系统也不熟悉，没在上面迭代开发过功能，是临时去帮忙救火的，对它的了解也只是停留在宏观层面，大概知道它的技术栈、架构、线上运行流程是怎么样的，下面给出跟定位问题相关的系统信息：

- 系统采用微服务架构，使用 Spring Cloud Alibaba 技术栈
- Nacos 注册中心、Gateway 网关、Sleuth 链路追踪、OpenFeign HTTP 远程调用
- Mybatis ORM 框架、Logback 日志实现
- Nginx 反向代理、 Linux 虚拟机
- 日志需要登录服务器看
- 登录鉴权使用 JWT
- Jenkins + Shell 脚本自动化部署
- 每个业务服务都是双节点部署

系统部署结构或者说运行时流程如下：
![](/assets/images/20220609/部署架构图.png)

如图所示，用户访问的时候，流量是先到 Nginx 反向代理服务器，由 Nginx 分发给 Gateway 网关，Gateway 网关再将流量分发给具体的业务服务，业务服务之间也会相互调用。

## 定位过程

我最开始接到的问题反馈只有一张报错的截图，一个团队成员向我求助说他搞不定，需要我帮忙看下，截图如下：
![](/assets/images/20220609/http_header_线上报错.png)

我第一次看这个报错截图的时候，看的不够仔细，第一反应就是这提示语不是已经很清楚了吗，请求头超出大小限制了，结合日志看下哪里报这个错的不就行了吗。

然而事实是图中 message 字段说的是 HTTP 协议头超出大小限制，没说是请求头还是响应头，是我想当然，惯性思维，先入为主了，需反思。

问了团队成员几个问题之后，我大概摸清了情况：

- 这是客户反馈的线上问题，是一个查询接口报错导致的，最近代码没改动过
- 同一个接口，请求参数除了某个参数值不一样，其它都一样，一个参数值是 1，一个参数值是 0，其中 1 会报错，0 不会
- 相同的请求参数，不同的登录用户，只有指定用户才会出现问题
- 测试环境无法重现问题，只有线上环境的指定账号能重现问题
- 只能看到网关的报错日志，业务服务的日志不知道什么原因没有打印日志

我登录服务器看了下网关的报错日志，试图从中找出什么蛛丝马迹，报错如下：
![](/assets/images/20220609/网关报错日志.png)

结合代码和网关日志，可以知道这个系统使用了 Sleuth 链路追踪，图中标红的地方就是由 Sleuth 生成的，拿着这个链路 id，我们可以把这个请求的网关日志和业务服务日志都捞出来，但是业务服务的日志没有打印，网关的日志中也没有什么有用的信息，线索在网关这里就断了。

我的团队成员也是卡在这里，但他在开始分析原因之前，拿报错信息去搜索引擎搜了下，按照别人的教程去改过网关和业务服务的 Nacos 配置，如下：

```yaml
#请求大小
spring.servlet.multipart.max-request-size=1024MB
#请求文件大小
spring.servlet.multipart.max-file-size=1024MB
```

这里先不去探讨这些配置是否合理，是否正确，最终这些配置是没能解决问题的，就我了解的知识来看，这些配置是针对 servlet 容器的，而 Gateway 网关使用的不是 servlet 容器，虽然业务服务使用了 servlet 容器，但是这些配置是针对请求的，而且从 multipart 这个关键字来看，似乎是针对文件上传请求的，对于我们遇到的 HTTP 协议头超出大小限制错误来说是否有效还得打个问号。

首先需要搞清楚到底是请求头大小超出了限制还是响应头大小超出了限制。通过从用户角度来操作，分别发起了一个正常请求和一个报错请求，其中两个请求的请求头都一样，请求参数除了某个参数值不一样之外，其它都一样，这样基本可以断定浏览器到网关之间请求头是没问题，因为同样的请求头，有正常的，也有报错的。

由于网关是可以随意修改请求的，包括请求头，网关有可能根据请求参数来加入额外的请求头，虽然这种可能性不大，因为业务逻辑一般不会写在网关，但我对这个系统不熟悉，为了严谨，我还是拿着前面提到的那个特殊请求参数来搜索网关的代码，搜不到，基本可以断定网关没有针对那个参数做特殊处理，也就可以断定网关到业务服务之间的请求头是没问题的。

那这样是不是可以断定报错的不是请求头导致的，而是响应头导致的呢？还不行，因为微服务之间可能会相互调用，而这个系统微服务之间调用使用的是 OpenFeign，协议正是 HTTP，所以还有可能是微服务之间调用的时候增加了额外的请求头导致。

为了排除这种可能，我当时是找到那个业务服务接口，直接分析代码，由于接口逻辑不复杂，接口里面就是简单的查库操作，很快就可以确定这个服务接口没有调用其它微服务接口，至此就可以下结论说报错不是请求头导致的，而是响应头导致的。

那么到底是哪里添加的响应头导致出错了呢？业务服务接口里面并没有涉及到响应头的操作，一种可能性是业务服务接口报错了，之后有某种异常处理机制介入，添加了响应头，然后这个响应头超出了网关的大小限制。

那该如何验证或者排除这种可能性呢？一种办法是我在本地开发环境将网关、业务服务都跑起来，修改接口的代码，模拟接口可能出现的各种错误，先直接调服务接口，看报错的响应头里面是否包含了额外的响应头，根据这个响应头的名字搜索代码，找到处理这个响应头的地方，然后再通过网关调服务接口，将响应头的数据长度填充到超过上面报错截图里面说的大小，看能不能重现问题，这样一番排查下来基本就可以知道是哪里出问题了。

但是这种方法成本太高，先不说需要去了解登录流程再调接口，模拟登录拿到凭证，仅是搭建这个运行环境，模拟各种错误，想办法验证假设，可能一两个小时就过去了，就算本地重现了同样错误，最后还是得去确认线上的报错和本地是不是同样原因导致的，还是离不开线上的业务服务日志。

所以当时我没有选择优先在本地搭建运行环境，分析代码，而是直接去解决线上日志打印的问题，大概扫了下项目的配置文件，是有配置打印日志到文件中的，但是服务器并没有打印日志到文件中，这时也来不及去分析原因了。

当时想到了由 Jenkins 触发执行的 Shell 自动部署脚本，找到了脚本里面的某行代码：

```bash
nohup java ${JAVA_OPTS} -jar ${jarname} ${JAR_OPTS} &> /dev/null &
```

这里涉及到了 Linux 的知识，这行脚本代码是利用 Linux 命令 nohup 将 Java 进程放在后台去跑，并且把标准输出流、标准错误流重定向到 /dev/null 文件中，这个文件是特殊文件，它就是一个黑洞，它会吐掉你给它的任何东西，Java 进程的日志除了会根据日志配置输出到指定文件之外，还会把日志输出到控制台。

运维在编写这个部署脚本的时候，把控制台的日志都重定向到了 /dev/null 文件中，也就是不记录控制台的日志。我们可以在这行部署代码上做手脚，将控制台日志临时输出到一个文件中，重启下业务服务就可以在指定文件中看到线上日志，这样的好处是不需要改代码和应用配置文件，也不需要重新编译打包，改了之后的代码如下：

```bash
nohup java ${JAVA_OPTS} -jar ${jarname} ${JAR_OPTS} &> ${logfile} &
```

不过这里需要提醒下，线上的服务不是我们想重启就重启，运维编写维护的脚本也不是我们想改就改的，这个还是要跟相关人员报备的。当时我首先是问业务方能否重启业务服务，业务方担心的是会影响到正常用户的使用，但是我给业务方打包票说不会影响到正常用户的使用，并跟他们说了我的方案，消除了业务方的顾虑；运维那边我也跟他说只是改下部署脚本临时记录日志，保证不会出问题，运维也知道改这个不会出问题的，就同意了。

这个重启服务的方案是这样的，在 Nacos 注册中心先把一个服务实例下线，让流量不再分发给这个实例，所有流量由另一个相同的服务实例来处理，然后重启这个服务实例，重新上线，等到重启的服务实例能正常处理流量了，再将另一个服务实例按照同样的方式重启，这样滚动重启是不会有什么问题的。

重启服务之后，线上重现问题就能在日志中看到异常栈了，通过结合代码分析异常栈得知服务接口中某条巨复杂的 SQL 查询语句利用了 Mybatis 的动态 SQL 功能编写了不少判断逻辑代码，在某种条件下生成的 SQL 语句 where 条件中缺失了关键字 or，导致执行的时候抛出 SQL 执行异常。

异常抛出后重定向到了业务服务的异常处理接口，在异常处理接口中对错误信息进行了封装，其中通过响应头返回错误信息给网关，由于封装的错误信息包含了复杂的 SQL 语句，超出了网关限制的响应头大小，导致网关解析响应头的时候报错，最后网关抛出 HTTP 协议头超出大小限制错误。
![](/assets/images/20220609/业务服务异常处理接口.png)

这里说下我是怎么分析得出这个结论的。通过分析业务服务抛出的异常栈，我们虽然知道是某条 SQL 语句执行报错了，但是要在复杂的 SQL 语句中定位出是哪个 Mybatis 判断逻辑生成的 SQL 导致的，我们免不了要在本地模拟数据去调试 SQL 语句，这个过程不懂得技巧的话，也会耗费不少时间在模拟环境和造数据上。

这里分享下我的技巧，既然知道是某条 SQL 语句导致的，那么我只需要专注于调试执行 SQL 的那一行代码，想办法组装 SQL 参数就行了，而不用去调试完整的业务流程，这个问题中执行 SQL 的 Java 代码是这样的：

```java
IPage<xxx>  page = lyricsSongClaimMapper.cpNotClaimQuery(req.page(), req, myClaimCpId, claimCpId, cpIds, allCpIds, cpLyricConfigRsp);
```

这里先忽略烂代码的问题，前面有说到不同的登录用户，只有特定的登录用户才有问题，原因就是这里的某些入参是从登录用户数据里面取的，这会影响到 Mybatis 最终生成的 SQL 语句。

既然用到了登录用户数据，本地调试的时候就需要造数据了，这个登录用户数据也有讲究的，我需要造那种会导致 SQL 报错的登录用户数据，那我怎么知道哪些数据会导致报错呢？前面也有提到测试环境很难重现问题。

一种办法是通过分析 Mybatis SQL 语句，把所有根据入参生成不同 SQL 的判断逻辑都走一遍，但这种办法对我来说时间成本有点高，我当时采取的是直接拿线上登录用户数据和线上的请求参数在本地组装参数，这样能最大程度还原线上出问题时的运行时环境，这又得去熟悉系统的登录流程了，看下登录数据是存储在哪里的。

如果是新手或者缺乏线上问题排查经验的人，跑去研究登录流程，想办法拿线上数据在本地调试不是最好的选择，因为他们很可能单是研究登录流程就研究大半天了，中间还可能会有各种卡点，还不如老老实实分析 Mybatis SQL 语句，反向造数据。

但对于我这种老兵来说，一听说系统使用 JWT 来做登录鉴权，脑海里面大概就知道登录流程是怎么样的，登录数据要么是加密到 Token 里面，要么是将 Token 作为 Key 存储到 Redis 里面，通过浏览器开发者工具的 Network 面板查看 HTTP 请求就能拿到这个 Token。这个系统的登录用户数据是加密到 Token 中的，这样只需要找到 Token 的密钥就可以在本地解密拿到线上的登录用户数据，而密钥是配置在 Nacos 注册中心的，登录生产的 Nacos 就能查到密钥。

经过不断分析和调试，最后得出上面说的结论，这里给下这条复杂 SQL 语句的部分代码：
![](/assets/images/20220609/Mybatis报错SQL.png)

## 总结

总结下问题定位过程的一些关键点：

1. 看到报错截图，通过询问相关人员了解情况，然后从用户角度操作，重现问题，这个是为了了解功能和业务流程。
2. 想到去看网关日志、业务服务日志，这个需要知道一个线上系统有哪些日志可以用，怎么配置的。
3. 从浏览器到 Nginx 到网关再到业务服务，使用对比分析排除法一步步确定问题发生区间，这个需要了解一个请求从端到端经过了哪些环节和中间件，也需要有 HTTP 协议基础。
4. 想到通过修改部署脚本，重定向输出流来记录日志，需要了解自动部署流程，还要有一定的 Linux 基础。
5. 想到通过线上 Token 来获取登录用户信息，需要了解 JWT 和常规登录流程。
6. 想到去 Nacos 注册中心拿 Token 密钥，需要知道微服务系统由哪些组件组成。
7. 想到通过链路 id 提取日志，需要了解链路追踪技术。
8. 想到通过滚动重启的方式重启服务，需要熟悉线上流量分发机制。

定位问题是一个思维发散的过程，技术视野和基础影响着思维发散程度，每次定位完一些奇奇怪怪的问题，都会加深我对相关技术的理解，也会带来一些新的疑问或者好奇心，比如这次，我就很好奇 Gateway 网关为什么限制响应头的大小，在哪个环节限制的，是否支持可配置等等。

这为我进一步学习 Gateway 网关提供了铺垫，这种问题驱动的学习方式是一种很好的学习方式。
